# vLLM ì„œë²„ ì„¤ì • ê°€ì´ë“œ

ì´ í”„ë¡œì íŠ¸ëŠ” vLLM ê¸°ë°˜ì˜ LLM ì„œë²„ë¥¼ Dockerë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì •í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

## ğŸ“¦ í”„ë¡œì íŠ¸ êµ¬ì„±

- `Dockerfile`: vLLM ì„œë²„ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ Docker ì´ë¯¸ì§€ ë¹Œë“œ
- `docker-compose.yaml`: ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ë° ê´€ë¦¬ë¥¼ ìœ„í•œ Docker Compose ì„¤ì •
- `install-nvidia-driver.sh`: í˜¸ìŠ¤íŠ¸ ë¨¸ì‹ ì— NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
- `setup-docker-nvidia.sh`: ë„ì»¤ ë° NVIDIA ì»¨í…Œì´ë„ˆ íˆ´í‚· ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
- `config/`: í™˜ê²½ ë³€ìˆ˜ ì„¤ì • íŒŒì¼ ë””ë ‰í† ë¦¬
  - `.env.2gpu.qwen80b`: Qwen 80B ëª¨ë¸ì„ ìœ„í•œ 2GPU í™˜ê²½ ì„¤ì • íŒŒì¼
  - `.env.4gpu.qwen235b`: Qwen 235B ëª¨ë¸ì„ ìœ„í•œ 4GPU í™˜ê²½ ì„¤ì • íŒŒì¼
  - `.env.4gpu.qwen480b.coder`: Qwen 480B Coder ëª¨ë¸ì„ ìœ„í•œ 4GPU í™˜ê²½ ì„¤ì • íŒŒì¼

## ğŸ› ï¸ ì‚¬ì „ ìš”êµ¬ ì‚¬í•­

### 1. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜

í˜¸ìŠ¤íŠ¸ ë¨¸ì‹ ì— NVIDIA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

```bash
# NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜
sudo ./install-nvidia-driver.sh

# ì„¤ì¹˜ í›„ ì‹œìŠ¤í…œ ì¬ë¶€íŒ…
sudo reboot
```

### 2. ë„ì»¤ ë° NVIDIA ì»¨í…Œì´ë„ˆ íˆ´í‚· ì„¤ì¹˜

```bash
# ë„ì»¤ ë° NVIDIA ì»¨í…Œì´ë„ˆ íˆ´í‚· ì„¤ì¹˜
sudo ./setup-docker-nvidia.sh

# ì„¤ì¹˜ í›„ ë¡œê·¸ì•„ì›ƒ í›„ ì¬ë¡œê·¸ì¸ (ë˜ëŠ” ì¬ë¶€íŒ…)
# SSH ì‚¬ìš© ì¤‘ì´ë©´ SSH ì¬ì ‘ì†

# ê·¸ë£¹ í™•ì¸
groups
# ì¶œë ¥ì— 'docker' ê·¸ë£¹ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
```

## ğŸ³ Dockerfile ì„¤ëª…

`Dockerfile`ì€ vLLM ì„œë²„ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤.

```dockerfile
FROM nvidia/cuda:12.8.0-devel-ubuntu24.04

# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    pkg-config libglvnd-dev dkms build-essential \
    libegl-dev libegl1 libgl-dev libgl1 libgles-dev libgles1 \
    libglvnd-core-dev libglx-dev libopengl-dev \
    gcc make screen nano isc-dhcp-client \
    python3-venv python3-pip wget git curl \
    && rm -rf /var/lib/apt/lists/*

# Python í™˜ê²½ ì„¤ì •
RUN python3 -m venv /vllm-env
ENV PATH="/vllm-env/bin:$PATH"

# PyTorch ë° NCCL ì„¤ì¹˜
RUN /vllm-env/bin/pip install --upgrade pip setuptools wheel && \
    /vllm-env/bin/pip install torch==2.8.0+cu128 torchvision --index-url https://download.pytorch.org/whl/cu128 && \
    /vllm-env/bin/pip install nvidia-nccl-cu12==2.27.3

# vLLM ì„¤ì¹˜
WORKDIR /
RUN git clone https://github.com/vllm-project/vllm.git
WORKDIR /vllm
RUN MAX_JOBS=32 /vllm-env/bin/pip install -e .

# ì‘ì—… ë””ë ‰í† ë¦¬
WORKDIR /vllm

CMD ["bash"]
```

ì£¼ìš” íŠ¹ì§•:
- `nvidia/cuda:12.8.0-devel-ubuntu24.04` ë² ì´ìŠ¤ ì´ë¯¸ì§€ ì‚¬ìš©
- Python ê°€ìƒ í™˜ê²½ ì„¤ì • (`/vllm-env`)
- PyTorch, torchvision, NCCL ì„¤ì¹˜
- vLLM í”„ë¡œì íŠ¸ í´ë¡  ë° ì„¤ì¹˜

## ğŸ“‹ docker-compose.yaml ì„¤ëª…

`docker-compose.yaml`ì€ ì»¨í…Œì´ë„ˆì˜ ì‹¤í–‰ í™˜ê²½ì„ ì •ì˜í•©ë‹ˆë‹¤.

```yaml
services:
  vllm:
    build:
      context: .
      dockerfile: Dockerfile
    image: vllm-qwen:latest
    container_name: vllm-qwen-server
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    ipc: host
    shm_size: 20gb
    
    ports:
      - "${VLLM_PORT}:${VLLM_PORT}"
    
    volumes:
      - ~/workspace:/workspace
      - ~/models:/models
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ~/.ssh:/root/.ssh:ro
    
    env_file:
      - .env
    
    environment:
      - OMP_NUM_THREADS=${OMP_NUM_THREADS}
      - MKL_NUM_THREADS=${MKL_NUM_THREADS}
      - OMP_PROC_BIND=${OMP_PROC_BIND}
      - OMP_PLACES=${OMP_PLACES}
      - VLLM_SLEEP_WHEN_IDLE=${VLLM_SLEEP_WHEN_IDLE}
      - VLLM_TUNE_FUSED_MOE=${VLLM_TUNE_FUSED_MOE}
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}
      - NCCL_DEBUG=${NCCL_DEBUG}
      - NCCL_P2P_DISABLE=${NCCL_P2P_DISABLE}
      - NCCL_IB_DISABLE=${NCCL_IB_DISABLE}
    
    command: >
      bash -c "
      source /vllm-env/bin/activate &&
      cd /vllm &&
      taskset -c ${TASKSET_CPUS} /vllm-env/bin/python3 /vllm-env/bin/vllm serve ${MODEL_NAME}
        --tensor-parallel-size ${TENSOR_PARALLEL_SIZE}
        --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION}
        --max-num-seqs ${MAX_NUM_SEQS}
        --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS}
        --max-model-len ${MAX_MODEL_LEN}
        --enable-prefix-caching
        --enable-chunked-prefill
        --async-scheduling
        --enable-auto-tool-choice
        --tool-call-parser hermes
        --disable-log-stats
        --port ${VLLM_PORT}
      "
    
    restart: unless-stopped
```

ì£¼ìš” ì„¤ì •:
- **GPU ì ‘ê·¼**: NVIDIA ë“œë¼ì´ë²„ë¥¼ í†µí•´ ëª¨ë“  GPU ì‚¬ìš©
- **IPC ëª¨ë“œ**: `host`ë¡œ ì„¤ì •í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹  ìµœì í™”
- **ê³µìœ  ë©”ëª¨ë¦¬**: 20GBë¡œ ì„¤ì •
- **í¬íŠ¸ ë§¤í•‘**: í™˜ê²½ë³€ìˆ˜ `${VLLM_PORT}`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì—°í•œ í¬íŠ¸ ì„¤ì •
- **ë³¼ë¥¨ ë§ˆìš´íŠ¸**: ë¡œì»¬ ë””ë ‰í† ë¦¬ë¥¼ ì»¨í…Œì´ë„ˆì— ë§ˆìš´íŠ¸
- **í™˜ê²½ë³€ìˆ˜**: `.env` íŒŒì¼ê³¼ í™˜ê²½ë³€ìˆ˜ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì„¤ì • ê°€ëŠ¥
- **ëª…ë ¹ì–´**: vLLM ì„œë²„ë¥¼ ìë™ìœ¼ë¡œ ì‹¤í–‰

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```bash
# .env íŒŒì¼ ì˜ˆì‹œ
VLLM_PORT=8000
MODEL_NAME=Qwen/Qwen2-72B-Instruct
TENSOR_PARALLEL_SIZE=8
GPU_MEMORY_UTILIZATION=0.95
MAX_NUM_SEQS=256
MAX_NUM_BATCHED_TOKENS=4096
MAX_MODEL_LEN=32768
TASKSET_CPUS=0-31
OMP_NUM_THREADS=16
MKL_NUM_THREADS=16
OMP_PROC_BIND=true
OMP_PLACES=cores
VLLM_SLEEP_WHEN_IDLE=true
VLLM_TUNE_FUSED_MOE=true
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
NCCL_DEBUG=INFO
NCCL_P2P_DISABLE=1
NCCL_IB_DISABLE=1
```

### 2. ì»¨í…Œì´ë„ˆ ë¹Œë“œ ë° ì‹¤í–‰

```bash
# ë¹Œë“œ ë° ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
docker compose up -d

# ë¡œê·¸ í™•ì¸
docker compose logs -f

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì— ì ‘ì†
docker exec -it vllm-qwen-server bash

# ì»¨í…Œì´ë„ˆ ì¤‘ì§€
docker compose down
```

### 3. ì„œë²„ í…ŒìŠ¤íŠ¸

```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8000/health

# ëª¨ë¸ ì •ë³´ í™•ì¸
curl http://localhost:8000/v1/models

# ì¶”ë¡  í…ŒìŠ¤íŠ¸
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2-72B-Instruct",
    "prompt": "Hello, how are you?",
    "max_tokens": 50
  }'
```

## ğŸ”„ ê°œë°œ ëª¨ë“œ

ê°œë°œ ì¤‘ì—ëŠ” ì»¨í…Œì´ë„ˆë¥¼ ì‰˜ ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì—¬ ë””ë²„ê¹…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# docker-compose.yamlì˜ commandë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •
command: ["bash"]

# ë˜ëŠ” docker run ëª…ë ¹ì–´ ì‚¬ìš©
docker run -it \
  --gpus all \
  --ipc=host \
  --shm-size=20g \
  -p 8000:8000 \
  -v ~/workspace:/workspace \
  -v ~/models:/models \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -v ~/.ssh:/root/.ssh:ro \
  --name gpu-dev \
  ubuntu:24.04 \
  sleep infinity \
  bash
```

## ğŸ¯ ì‚¬ì „ ì •ì˜ëœ í™˜ê²½ ì„¤ì •

### Qwen 80B ëª¨ë¸ (2GPU)

`config/.env.2gpu.qwen80b` íŒŒì¼ì€ Qwen 80B ëª¨ë¸ì„ 2ê°œì˜ GPUì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ìµœì ì˜ ì„¤ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.

```bash
# Qwen 80B ëª¨ë¸ì„ ìœ„í•œ 2GPU í™˜ê²½ ì„¤ì •
# ì‚¬ìš© ë°©ë²•: docker compose --env-file config/.env.2gpu.qwen80b up -d

# CPU ìŠ¤ë ˆë“œ ì„¤ì •
OMP_NUM_THREADS=32
MKL_NUM_THREADS=32
OMP_PROC_BIND=true
OMP_PLACES=cores

# vLLM ì„¤ì •
VLLM_SLEEP_WHEN_IDLE=1
VLLM_TUNE_FUSED_MOE=1

# GPU ì„¤ì •
CUDA_VISIBLE_DEVICES=0,1

# NCCL ì„¤ì •
NCCL_DEBUG=WARN
NCCL_P2P_DISABLE=0
NCCL_IB_DISABLE=1

# ëª¨ë¸ ì„¤ì •
MODEL_NAME=Qwen/Qwen3-Next-80B-A3B-Instruct-FP8
TENSOR_PARALLEL_SIZE=2
GPU_MEMORY_UTILIZATION=0.90
MAX_NUM_SEQS=64
MAX_NUM_BATCHED_TOKENS=65535
MAX_MODEL_LEN=65535

# í¬íŠ¸ ì„¤ì •
VLLM_PORT=8000

# CPU ì½”ì–´ í• ë‹¹ (taskset)
TASKSET_CPUS=0-63
```

**ì‚¬ìš© ë°©ë²•:**
```bash
docker compose --env-file config/.env.2gpu.qwen80b up -d
```

### Qwen 235B ëª¨ë¸ (4GPU)

`config/.env.4gpu.qwen235b` íŒŒì¼ì€ Qwen 235B ëª¨ë¸ì„ 4ê°œì˜ GPUì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ìµœì ì˜ ì„¤ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.

```bash
# Qwen 235B ëª¨ë¸ì„ ìœ„í•œ 4GPU í™˜ê²½ ì„¤ì •
# ì‚¬ìš© ë°©ë²•: docker compose --env-file config/.env.4gpu.qwen235b up -d

# CPU ìŠ¤ë ˆë“œ ì„¤ì •
OMP_NUM_THREADS=32
MKL_NUM_THREADS=32
OMP_PROC_BIND=true
OMP_PLACES=cores

# vLLM ì„¤ì •
VLLM_SLEEP_WHEN_IDLE=1
VLLM_TUNE_FUSED_MOE=1

# GPU ì„¤ì •
CUDA_VISIBLE_DEVICES=0,1,2,3

# NCCL ì„¤ì •
NCCL_DEBUG=WARN
NCCL_P2P_DISABLE=0
NCCL_IB_DISABLE=1

# ëª¨ë¸ ì„¤ì •
MODEL_NAME=Qwen/Qwen3-235B-A22B-Instruct-2507-FP8
TENSOR_PARALLEL_SIZE=4
GPU_MEMORY_UTILIZATION=0.90
MAX_NUM_SEQS=64
MAX_NUM_BATCHED_TOKENS=98304
MAX_MODEL_LEN=131072

# í¬íŠ¸ ì„¤ì •
VLLM_PORT=8000

# CPU ì½”ì–´ í• ë‹¹ (taskset)
TASKSET_CPUS=0-63
```

**ì‚¬ìš© ë°©ë²•:**
```bash
docker compose --env-file config/.env.4gpu.qwen235b up -d
```

### Qwen 480B Coder ëª¨ë¸ (4GPU)

`config/.env.4gpu.qwen480b.coder` íŒŒì¼ì€ Qwen 480B Coder ëª¨ë¸ì„ 4ê°œì˜ GPUì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ìµœì ì˜ ì„¤ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.

```bash
# Qwen 480B Coder ëª¨ë¸ì„ ìœ„í•œ 4GPU í™˜ê²½ ì„¤ì •
# ì‚¬ìš© ë°©ë²•: docker compose --env-file config/.env.4gpu.qwen480b.coder up -d

# CPU ìŠ¤ë ˆë“œ ì„¤ì •
OMP_NUM_THREADS=32
MKL_NUM_THREADS=32
OMP_PROC_BIND=true
OMP_PLACES=cores

# vLLM ì„¤ì •
VLLM_SLEEP_WHEN_IDLE=1
VLLM_TUNE_FUSED_MOE=1

# GPU ì„¤ì •
CUDA_VISIBLE_DEVICES=0,1,2,3

# NCCL ì„¤ì •
NCCL_DEBUG=WARN
NCCL_P2P_DISABLE=0
NCCL_IB_DISABLE=1

# ëª¨ë¸ ì„¤ì •
MODEL_NAME=Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8
TENSOR_PARALLEL_SIZE=4
GPU_MEMORY_UTILIZATION=0.90
MAX_NUM_SEQS=64
MAX_NUM_BATCHED_TOKENS=98304
MAX_MODEL_LEN=131072

# í¬íŠ¸ ì„¤ì •
VLLM_PORT=8000

# CPU ì½”ì–´ í• ë‹¹ (taskset)
TASKSET_CPUS=0-63
```

**ì‚¬ìš© ë°©ë²•:**
```bash
docker compose --env-file config/.env.4gpu.qwen480b.coder up -d
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/)
- [Docker Compose ë¬¸ì„œ](https://docs.docker.com/compose/)